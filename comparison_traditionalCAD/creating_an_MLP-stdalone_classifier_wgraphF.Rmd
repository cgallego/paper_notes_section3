---
title: "Creating an MLP-stdalone classifier with Graph features - nonmass MRI findings (only labeled)"
output: 
  html_document: 
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    theme: cosmo
    toc: yes
---

## Functions
The following functions are included to creating an MLP-stdalone classifier with subset_select and parameter search for "# hidden layers" and "# of nodes in hidden layers" and 5f-cv resampling:
* cvfold_partition
* kparti_sample
* subset_select
* calcAUC_plot
* surface_forestperfm

```{r funcs, echo=FALSE, eval=TRUE, message = FALSE}
###################################################
### code to sample kparti from a cross-validation set up: 
### kparti = k fold to exclude
### outs: cvTrainsetD, cvTestsetD
###################################################
cvfold_partition <- function(dat, cvfoldK){
  library(caret)
  ndat = nrow(dat)
  outcomesetDi  <- dat$labels
  #For multiple k-fold cross-validation, completely independent folds are created.
  #when y is a factor in an attempt to balance the class distributions within the splits.
  #The names of the list objects will denote the fold membership using the pattern 
  #"Foldi.Repj" meaning the ith section (of k) of the jth cross-validation set (of times).
  partitionsetDi <- createFolds(y = outcomesetDi, ## the outcome data are needed
                                k = cvfoldK, ## The percentage of data in the training set
                                list = TRUE) ## The format of the results. 
  return(partitionsetDi)
}

kparti_sample <- function(dat, particvfoldK, cvfoldK, kparti){
  allparti = 1:cvfoldK
  allbutkparti = allparti[-kparti]
  cvfoldadd = c()
  for(i in 1:length(allbutkparti)){
    kadd = allbutkparti[i]
    cvfoldadd = c(cvfoldadd, particvfoldK[[kadd]])
  }
  # partition data
  cvTrainsetD <-  dat[ cvfoldadd ,]
  cvValsetD <-   dat[-cvfoldadd ,]
  
  output <- list(cvTrainsetD=cvTrainsetD, cvValsetD=cvValsetD)
  return(output)
}

###################################################
### code to perform supervised Feature selection wiht boruta
###statsAU
###################################################
subset_select <- function(dat){
  library(Boruta)
  borutadat = na.omit(dat)
  featsel_boruta <-Boruta(x=dat[,2:ncol(dat)], y=dat$labels, doTrace=1, ntree=1000)
  print(featsel_boruta)
  #plot(featsel_boruta)
  
  relevant <- featsel_boruta$finalDecision[featsel_boruta$finalDecision == "Confirmed"]
  relevant_features = dat[c(names(relevant))]
  tentative <- featsel_boruta$finalDecision[featsel_boruta$finalDecision == "Tentative"]
  tentative_features = dat[c(names(tentative))]
  sel_features = cbind(dat[c(1,2)], relevant_features, tentative_features)
  
  return(sel_features)
}

################ 
## Calculate and plot an ROC with CI and optimal threshold
################ 
calcAUC_plot <- function(obs, probpred, xptext, yptext, icolors, printauc, atitle){
  library(pROC)
  ROC <- plot.roc(obs, 
                  probpred,
                  ci=TRUE, print.auc=printauc, print.auc.x=xptext, print.auc.y=yptext,
                  col=icolors, lty=1, legacy.axes=TRUE,
                  main=atitle)
  # determine best operating point (maximizes both Se Spe)
  # ìbestî: the threshold with the highest sum sensitivity + specificity is plotted (this might be more than one threshold).
  best_thr=ci(ROC, of="thresholds", thresholds="best")
  plot(best_thr, col=icolors) # add one threshold
  #print(ROC$auc)
  #print(best_thr)
  output <- list(ROC=ROC, auc=ROC$auc, best_thr=best_thr)
  return(output)
}

###################################################
### code to plot learning surfaces ncol=2
###################################################
surface_forestperfm <- function(grdperf, ncol, dim1, dim2){
  library(gridExtra) 
  library(base)
  library(lattice)
  
  graphlist<-list()
  count <- 1
  # design rocTrain
  z = grdperf[,1]
  gD=unique(grdperf[,c(dim1)])
  gT=unique(grdperf[,c(dim2)])
  dim(z) <- c(length(gD), length(gT))
  w1 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = dim1,
                  ylab = dim2,
                  main = "Influence of parameters on ROC AUC 5f.cv Train",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  zlim=c(0.5,1),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w1
  count <- count+1
  
  # design rocTest
  z = grdperf[,2]
  dim(z) <- c(length(gD), length(gT))
  w2 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = dim1,
                  ylab = dim2,
                  main = "Influence of parameters on ROC AUC 5f.cv Val",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  zlim=c(0.5,1),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w2
  count <- count+1
  
  # finally plot in grid
  do.call("grid.arrange",c(graphlist,ncol=ncol))
}

###################################################
```


## Creating an MLP-stdalone classifier to compare with MLP + DEC unsupervised learning
```{r echo=TRUE, eval=TRUE, message = FALSE, warning=FALSE}
# Read CSV into R
library(neuralnet)
library(pROC)
setwd("Z:/Cristina/Section3/paper_notes/comparison_traditionalCAD")
source('functionsCAD.R')
pdatalabels <- read.csv(file="input/pdatalabels.csv", header=TRUE, sep=",")
dyn_roi <- read.csv(file="input/dyn_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")
morpho_roi <- read.csv(file="input/morpho_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")
text_roi <- read.csv(file="input/text_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")
stage1_roi <- read.csv(file="input/stage1_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")

nxGnorm <- read.csv(file="input/nxGnormfeatures_allNMEs_descStats.csv", header=FALSE, sep=",")
colnames(nxGnorm) <- paste("nxg", c(1:ncol(nxGnorm)),  sep ="")

# append all with lables
allfeatures = data.frame(cbind(pdatalabels,
                               dyn_roi[2:ncol(dyn_roi)],
                               morpho_roi[2:ncol(morpho_roi)],
                               text_roi[2:ncol(text_roi)],
                               stage1_roi[2:ncol(stage1_roi)]))

# print summary labesl
summary(allfeatures$labels)

# remove unlabeled or 'K'
# print summary of lesions in dataset:
onlylab_nxG =  cbind(allfeatures[allfeatures$labels!='K',], nxGnorm[allfeatures$labels!='K',])
onlylab_nxG$labels = factor(onlylab_nxG$labels)
summary(onlylab_nxG$labels)

## normalize data before training a neural network on it
onlylab_nxG$dce2SE19[is.na(onlylab_nxG$dce2SE19)] <- summary(onlylab_nxG$dce2SE19)[[4]]
onlylab_nxG$dce3SE19[is.na(onlylab_nxG$dce3SE19)] <- summary(onlylab_nxG$dce3SE19)[[4]]
onlylab_nxG$earlySE19[is.na(onlylab_nxG$earlySE19)] <- summary(onlylab_nxG$earlySE19)[[4]]

## normalize data before training a neural network on it
maxs <- apply(onlylab_nxG[c(2:ncol(onlylab_nxG))], 2, max)
mins <- apply(onlylab_nxG[c(2:ncol(onlylab_nxG))], 2, min)

Xscaled <- as.data.frame(scale(onlylab_nxG[c(2:ncol(onlylab_nxG))], center = mins, scale = maxs - mins))
data = cbind(Xscaled,labels=ifelse(onlylab_nxG$labels=='M',1,0))

## split in 90%/10% train/test 
sep = round(nrow(allfeatures)*0.10)
X_test = data[1:sep,]
y_test = X_test$labels
init_indTrain = sep+1
X_train = data[init_indTrain:nrow(data),]
y_train = X_train$labels

###################################################
### Train a CAD classifier using traditional CAD features (only supervised features)
###################################################
## create stratified kfolds
cvK = 5
particvfoldK = cvfold_partition(X_train, cvK)

###################################################
# create grid of evaluation points
gh1 = c(128,64) 
gh2 = c(0,32) 
grd1 <- expand.grid(h1 = gh1, h2 = gh2)
gh3 = c(48,24) 
gh4 = c(0,10)
grd2 <- expand.grid(h1 = gh3, h2 = gh4)
grd = rbind(grd1,grd2)

###################################################
# initialize grid search metrics
grdperf = data.frame(grd)
grdperf$avaucTrain =0
grdperf$stdTrain =0
grdperf$avaucVal =0
grdperf$stdVal =0

for(k in 1:nrow(grd)){
  # get grid search cell
  H = c(grd[k,][[1]],grd[k,][[2]])
  # Build in l
  cat("#h1: ", H[1], "#h2: ", H[2], "\n")
  cvAUC_train = c()
  cvAUC_val = c()
  for(r in 1:cvK){
    ## pick one of cvfold for held-out test, train on the rest
    kparti_setdata = kparti_sample(X_train, particvfoldK, cvK, r)
    
    # Boruta on $cvTrainsetD
    selfeatures_kfold = subset_select(kparti_setdata$cvTrainsetD)
    names(selfeatures_kfold)
    
    # train classifier in train with featset in train
    TrainsetD <-  kparti_setdata$cvTrainsetD[c(names(selfeatures_kfold))]
    ValsetD <-  kparti_setdata$cvValsetD[c(names(selfeatures_kfold))]
    
    # CREATE FORMULA  for grid search parameters train
    feats <- names(selfeatures_kfold[,!(names(selfeatures_kfold) %in% "labels")])
    # Concatenate strings
    f <- paste(feats,collapse=' + ')
    f <- paste('labels ~',f)
    # Convert to formula
    f <- as.formula(f)
    print(f)
    
    #install.packages('neuralnet')
    nn <- neuralnet(f, TrainsetD, hidden=c(H[H!=0]), linear.output=FALSE)
    
    # Compute Predictions off Test Set
    train.nn.values <- compute(nn, TrainsetD[,!(names(TrainsetD) %in% "labels")])
    valid.nn.values <- compute(nn, ValsetD[ , !(names(ValsetD) %in% "labels")])

    # have results between 0 and 1 that are more like probabilities of belonging to each class. We'll use sapply() to round these off to either 0 or 1 class so we can evaluate them against the test labels.
    train.nn.values$round.result <- sapply(train.nn.values$net.result,round,digits=0)
    valid.nn.values$round.result <- sapply(valid.nn.values$net.result,round,digits=0)
    
    # for train
    ROC_train <- roc(TrainsetD$labels, train.nn.values$net.result, plot=FALSE)
    ROC_val <- roc(ValsetD$labels, valid.nn.values$net.result, plot=FALSE)
    print(paste0("ROC_train$auc = ",ROC_train$auc))
    print(paste0("ROC_val$auc = ",ROC_val$auc))
    # appends
    cvAUC_train = c(cvAUC_train, ROC_train$auc)
    cvAUC_val = c(cvAUC_val, ROC_val$auc)
  }
  
  # collect data
  grdperf$avaucTrain[k] = mean(cvAUC_train)
  grdperf$stdTrain[k] = sd(cvAUC_train)
  grdperf$avaucVal[k] = mean(cvAUC_val)
  grdperf$stdVal[k] = sd(cvAUC_val)
}

print(grdperf)

################
# select best grid parameters
index = which(grdperf$avaucVal == max(grdperf$avaucVal), arr.ind = TRUE)[1]
h1 = grdperf$h1[index]
h2 = grdperf$h2[index]
print(grdperf[index,])

# # predict and evaluate performance
# assess on held out test
################
# to pool data
train_pr=data.frame()
val_pr=data.frame()
for(r in 1:cvK){
  ## pick one of cvfold for held-out test, train on the rest
  kparti_setdata = kparti_sample(X_train, particvfoldK, cvK, r)
  
  # Boruta on $cvTrainsetD
  selfeatures = subset_select(kparti_setdata$cvTrainsetD)
  names(selfeatures)
  
  # train classifier in train with featset in train
  TrainsetD <-  kparti_setdata$cvTrainsetD[c(names(selfeatures))]
  ValsetD <-  kparti_setdata$cvValsetD[c(names(selfeatures))]
  
  # CREATE FORMULA  for grid search parameters train
  feats <- names(selfeatures[,!(names(selfeatures) %in% "labels")])
  # Concatenate strings
  f <- paste(feats,collapse=' + ')
  f <- paste('labels ~',f)
  # Convert to formula
  f <- as.formula(f)
  print(f)
  
  #install.packages('neuralnet')
  H = c(h1,h2)
  nn <- neuralnet(f, TrainsetD, hidden=c(H[H!=0]), linear.output=FALSE)

  # Compute Predictions off Test Set
  train.nn.values <- compute(nn, TrainsetD[,!(names(TrainsetD) %in% "labels")])
  val.nn.values <- compute(nn, ValsetD[ , !(names(ValsetD) %in% "labels")])
  
  # have results between 0 and 1 that are more like probabilities of belonging to each class. We'll use sapply() to round these off to either 0 or 1 class so we can evaluate them against the test labels.
  train.nn.values$round.result <- sapply(train.nn.values$net.result,round,digits=0)
  val.nn.values$round.result <- sapply(val.nn.values$net.result,round,digits=0)
  
  ## Now let's create a simple confusion matrix:
  table(TrainsetD$labels,train.nn.values$round.result)
  table(ValsetD$labels,val.nn.values$round.result)
  
  # for train
  ROC_train <- roc(TrainsetD$labels, train.nn.values$net.result, plot=FALSE)
  ROC_test <- roc(ValsetD$labels, val.nn.values$net.result, plot=FALSE)
  print(paste0("ROC_train$auc = ",ROC_train$auc))
  print(paste0("ROC_test$auc = ",ROC_test$auc))
  # append
  trainprob = cbind(TrainsetD$labels,train.nn.values$net.result)
  valprob = cbind(ValsetD$labels,val.nn.values$net.result)
  train_pr = rbind(train_pr, trainprob)
  val_pr = rbind(val_pr, valprob)
}

## plot ROC 
plot.new()
ROCcv_train <- plot.roc(train_pr$V1, train_pr$V2, col="#000086", lty=1)
ciobj <- ci.se(ROCcv_train, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
ROCcv_train <- plot.roc(train_pr$V1, train_pr$V2, col="#000086", lty=1, main="ROC for cvTrain")
legend("bottomright", 
       legend = c(paste0("cvTrain: AUC=", formatC(ROCcv_train$auc,digits=2, format="f"))), 
       col = c("#000086"),lwd = 2, lty = c(1))

plot.new()
ROCcv_val <- plot.roc(val_pr$V1, val_pr$V2, col="#008600", lty=1)
ciobj <- ci.se(ROCcv_val, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
ROCcv_val <- plot.roc(val_pr$V1, val_pr$V2, col="#008600", lty=1, main="ROC for cvVal")
legend("bottomright", 
       legend = c(paste0("cvVal: AUC=", formatC(ROCcv_val$auc,digits=2, format="f"))), 
       col = c("#008600"),lwd = 2, lty = c(1))


# # predict and evaluate performance
# assess on held out test by training in all train data
# Boruta on $cvTrainsetD
selfeatures = subset_select(X_train)
names(selfeatures)

# train classifier in train with featset in train
TrainsetD <-  X_train[c(names(selfeatures))]
TestsetD <-  X_test[c(names(selfeatures))]

# CREATE FORMULA  for grid search parameters train
feats <- names(selfeatures[,!(names(selfeatures) %in% "labels")])
# Concatenate strings
f <- paste(feats,collapse=' + ')
f <- paste('labels ~',f)
# Convert to formula
f <- as.formula(f)
print(f)

H = c(h1,h2)
nn <- neuralnet(f, TrainsetD, hidden=c(H[H!=0]), linear.output=FALSE)

# Compute Predictions off Test Set
train.nn.values <- compute(nn, TrainsetD[,!(names(TrainsetD) %in% "labels")])
test.nn.values <- compute(nn, TestsetD[ , !(names(TestsetD) %in% "labels")])

# have results between 0 and 1 that are more like probabilities of belonging to each class. We'll use sapply() to round these off to either 0 or 1 class so we can evaluate them against the test labels.
train.nn.values$round.result <- sapply(train.nn.values$net.result,round,digits=0)
test.nn.values$round.result <- sapply(test.nn.values$net.result,round,digits=0)

plot.new()
ROC_test <- plot.roc(TestsetD$labels, test.nn.values$round.result, col="#860000", lty=1, main="ROC for held-out test")
legend("bottomright", 
       legend = c(paste0("Test: AUC=", formatC(ROC_test$auc,digits=2, format="f"))), 
       col = c("#860000"),lwd = 2, lty = c(1))

print(ROCcv_train$auc)
print(ROCcv_val$auc)
print(ROC_test$auc)

## significance testing between AUCs
roc.test(ROCcv_train,ROCcv_val,method="bootstrap",boot.stratified = TRUE, alternative="greater")
roc.test(ROCcv_val,ROC_test,method="bootstrap",boot.stratified = TRUE, alternative="two.sided")

###############################
###############################
### now with DEC +MLP classifier
pdZ_grdperf <- read.csv(file="input/pdAUC_Zlatent.csv", header=TRUE, sep=",")
print(pdZ_grdperf)

################
# plot learning surface of ensemble parameters
# assumes the first two columns are AUCtrain, AUCtest
surface_forestperfm(pdZ_grdperf, 2, "spaceD_Redx", "num_clusters")

pdZ_pooled_pred_train <- read.csv(file="input/pooled_pred_train.csv", header=TRUE, sep=",")
pdZ_pooled_pred_val <- read.csv(file="input/pooled_pred_val.csv", header=TRUE, sep=",")
pdZ_test <- read.csv(file="input/pdZ_test.csv", header=TRUE, sep=",")
pdZ_pooled_pred_train$labels = factor(ifelse(pdZ_pooled_pred_train$labels==1,"M","B"))
pdZ_pooled_pred_val$labels = factor(ifelse(pdZ_pooled_pred_val$labels==1,"M","B"))
pdZ_test$labels = factor(ifelse(pdZ_test$labels==1,"M","B"))

## plot ROC 
plot.new()
Z_ROCcv_train <- plot.roc(pdZ_pooled_pred_train$labels, pdZ_pooled_pred_train$probC, col="#000086", lty=1)
ciobj <- ci.se(Z_ROCcv_train, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
Z_ROCcv_train <- plot.roc(pdZ_pooled_pred_train$labels, pdZ_pooled_pred_train$probC, col="#000086", lty=1, main="ROC for cvTrain")
legend("bottomright", 
       legend = c(paste0("cvTrain: AUC=", formatC(Z_ROCcv_train$auc,digits=2, format="f"))), 
       col = c("#000086"),lwd = 2, lty = c(1))

plot.new()
Z_ROCcv_val <- plot.roc(pdZ_pooled_pred_val$labels, pdZ_pooled_pred_val$probC, col="#008600", lty=1)
ciobj <- ci.se(Z_ROCcv_val, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
Z_ROCcv_val <- plot.roc(pdZ_pooled_pred_val$labels, pdZ_pooled_pred_val$probC, col="#008600", lty=1, main="ROC for cvVal")
legend("bottomright", 
       legend = c(paste0("cvVal: AUC=", formatC(Z_ROCcv_val$auc,digits=2, format="f"))), 
       col = c("#008600"),lwd = 2, lty = c(1))

plot.new()
Z_ROCtest <- plot.roc(pdZ_test$labels, pdZ_test$probC, col="#860000", lty=1, main="ROC for held-out")
legend("bottomright", 
       legend = c(paste0("Test: AUC=", formatC(Z_ROCtest$auc,digits=2, format="f"))), 
       col = c("#860000"),lwd = 2, lty = c(1))

print(Z_ROCcv_train$auc)
print(Z_ROCcv_val$auc)
print(Z_ROCtest$auc)

## significance testing between AUCs
roc.test(Z_ROCcv_train, Z_ROCcv_val, method="bootstrap",boot.stratified = TRUE, alternative="greater")
roc.test(Z_ROCcv_train, Z_ROCtest, method="bootstrap",boot.stratified = TRUE, alternative="greater")

# compare two methods on pooled predictions from all data
roc.test(ROCcv_val, Z_ROCcv_val, method="bootstrap",boot.stratified = TRUE, alternative="less")


save.image("Z:/Cristina/Section3/paper_notes/comparison_traditionalCAD/traditional_MLP_wgraph_CADperf.RData")

```

