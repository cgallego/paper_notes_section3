---
title: "Creating an ensemble of RF - nonmass MRI findings with graph features (only labeled)"
output: 
  html_document: 
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    theme: cosmo
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Functions

The following functions are included to create ensembles of RF with subset_select and parameter search for "Depth" and "ntrees" and 5f-cv resampling:
* cvfold_partition
* kparti_sample
* subset_select
* rpart_looforestTrain
* rpart_looforestTest
* calcAUC_plot
* surface_forestperfm
* create_ensemble (previous non-mass ensemble creation - see main code)

```{r funcs, echo=FALSE, eval=TRUE, message = FALSE, warning=FALSE}
###################################################
### code to sample kparti from a cross-validation set up: 
### kparti = k fold to exclude
### outs: cvTrainsetD, cvTestsetD
###################################################
cvfold_partition <- function(dat, cvfoldK){
  library(caret)
  ndat = nrow(dat)
  outcomesetDi  <- dat$labels
  #For multiple k-fold cross-validation, completely independent folds are created.
  #when y is a factor in an attempt to balance the class distributions within the splits.
  #The names of the list objects will denote the fold membership using the pattern 
  #"Foldi.Repj" meaning the ith section (of k) of the jth cross-validation set (of times).
  partitionsetDi <- createFolds(y = outcomesetDi, ## the outcome data are needed
                                k = cvfoldK, ## The percentage of data in the training set
                                list = TRUE) ## The format of the results. 
  return(partitionsetDi)
}

kparti_sample <- function(dat, particvfoldK, cvfoldK, kparti){
  allparti = 1:cvfoldK
  allbutkparti = allparti[-kparti]
  cvfoldadd = c()
  for(i in 1:length(allbutkparti)){
    kadd = allbutkparti[i]
    cvfoldadd = c(cvfoldadd, particvfoldK[[kadd]])
  }
  # partition data
  cvTrainsetD <-  dat[ cvfoldadd ,]
  cvValsetD <-   dat[-cvfoldadd ,]
  
  output <- list(cvTrainsetD=cvTrainsetD, cvValsetD=cvValsetD)
  return(output)
}

###################################################
### code to perform supervised Feature selection wiht boruta
###statsAU
###################################################
subset_select <- function(dat){
  library(Boruta)
  borutadat = na.omit(dat)
  featsel_boruta <-Boruta(x=dat[,2:ncol(dat)], y=dat$labels, doTrace=1, ntree=1000)
  print(featsel_boruta)
  #plot(featsel_boruta)
  
  relevant <- featsel_boruta$finalDecision[featsel_boruta$finalDecision == "Confirmed"]
  relevant_features = dat[c(names(relevant))]
  tentative <- featsel_boruta$finalDecision[featsel_boruta$finalDecision == "Tentative"]
  tentative_features = dat[c(names(tentative))]
  sel_features = cbind(dat[c(1,2)], relevant_features, tentative_features)
  
  return(sel_features)
}

###################################################
### code forest Train: 
### parameters, T= # of trees, D= tree depth, dat
###################################################
# bagged training was introduced as a way of reducing possible overfitting and 
# improving the generalization capabilities of random forests. 
# The idea is to train each tree in a forest on a different training subset, sampled at random from the same labeled database. 
rpart_looforestTrain <- function(T, D, dat) {
  library(rpart)
  library(rpart.plot)
  # set control
  fitparm = rpart.control(maxdepth = D, minsplit = 9, minbucket = 3, cp = 0.01,  xval = 5,
                          maxcompete = 0, maxsurrogate = 0, usesurrogate = 0, surrogatestyle = 0)
  
  # init forest
  forest = list()
  for (t in 1:T){
    # build bagged trees from a bootstrap sample of trainSetD
    setD = dat[sample(1:nrow(dat), nrow(dat), replace=TRUE),]
    
    # find subsample of var
    # when training the ith tree we only make available a small random subset 
    subvar = sample(2:ncol(setD), sqrt(ncol(setD)-1), replace = FALSE)
    subfeat = colnames(setD)[subvar]
    
    # train tree
    treedata <- rpart(paste("labels ~ ", paste(subfeat, collapse= "+")), 
                      method = "class", data = setD, control=fitparm)
    
    # display the probability per class of observations in the node (conditioned on the node, sum across a     node is 1) plus the percentage of observations in the node. 
    if (T==1){
      print(treedata)
      prp(treedata, type=2, digits=3, extra = 102, under=TRUE, nn=TRUE, col="black", 
          box.col=rainbow(2)[2], varlen=0, faclen=0, branch.type=0, gap=0, cex=.7,
          fallen.leaves=TRUE) # use fallen.leaves=TRUE, to plot at bottom  
    }  
    
    # append
    forest <- append(forest, list(tree = treedata))    
  }
  
  output <- list(forest=forest)
  return(output)
}


###################################################
### code forest Test: 
### parameters, T= # of trees, forest, TrainsetD, TestsetD
###################################################
rpart_looforestTest <- function(T, TrainsetD, TestsetD, forest) {
  # TrainsetD = TrainsetD[c(2:ncol(TrainsetD))]
  # TestsetD = TestsetD[c(2:ncol(TestsetD))]
  # forest = fit$forest
  fclasspotrain=list()
  for (t in 1:T){
    # Calcultate posterior Probabilities on grid points
    temp <- predict(forest[t]$tree, newdata = TrainsetD) #
    fclasspotrain <- append(fclasspotrain, list(cpo = temp))
  }
  
  # run testing cases
  fclasspotest=list()
  for (t in 1:T){
    # Calcultate posterior Probabilities on grid points
    temp <- predict(forest[t]$tree, newdata = TestsetD) #
    fclasspotest <- append(fclasspotest, list(cpo = temp))
  }
  
  # performance on Train/Test set separately
  # extract ensamble class probabilities (when T > 1)
  trainpts = fclasspotrain[1]$cpo
  testpts = fclasspotest[1]$cpo
  # init ensample class posteriors
  enclasspotrain <- matrix(, nrow = nrow(as.data.frame(trainpts)), ncol = 2)
  enclasspotest <- matrix(, nrow = nrow(as.data.frame(testpts)), ncol = 2)
  enclasspotrain[,1] = fclasspotrain[1]$cpo[,1]
  enclasspotest[,1] = fclasspotest[1]$cpo[,1]
  enclasspotrain[,2] = fclasspotrain[1]$cpo[,2]
  enclasspotest[,2] = fclasspotest[1]$cpo[,2]
  if(T>=2){
    for (t in 2:T){
      #train
      enclasspotrain[,1] = enclasspotrain[,1]+fclasspotrain[t]$cpo[,1]
      enclasspotrain[,2] = enclasspotrain[,2]+fclasspotrain[t]$cpo[,2]
      #test
      enclasspotest[,1] = enclasspotest[,1]+fclasspotest[t]$cpo[,1]
      enclasspotest[,2] = enclasspotest[,2]+fclasspotest[t]$cpo[,2]
    }
  }
  # majority voting averaging
  enclasspotrain = (1/T)*enclasspotrain
  enclasspotest = (1/T)*enclasspotest
  
  # on training
  classes = levels(TrainsetD$labels)
  trainprob = data.frame(C1=enclasspotrain[,1],
                         C2=enclasspotrain[,2],
                         pred=classes[apply(enclasspotrain, 1, which.max)], 
                         obs=TrainsetD$labels)
  colnames(trainprob)[1:2] <- classes
  pred=as.factor(apply(enclasspotrain, 1, which.max))
  levels(pred) = levels(as.factor(TrainsetD$labels))
  perf_train = confusionMatrix(pred, as.factor(TrainsetD$labels))
  #print(perf_train)
  
  # on testing
  testprob = data.frame(C1=enclasspotest[,1],
                        C2=enclasspotest[,2],
                        pred=classes[apply(enclasspotest, 1, which.max)], 
                        obs=TestsetD$labels)
  colnames(testprob)[1:2] <- classes
  pred=as.factor(apply(enclasspotest, 1, which.max))
  levels(pred) = levels(as.factor(TrainsetD$labels))
  groundT = as.factor(TestsetD$labels)
  levels(groundT) = levels(as.factor(TrainsetD$labels))
  perf_test = confusionMatrix(pred, groundT)
  #print(perf_test)  
  
  output <- list(etrain = perf_train, etest=perf_test, trainprob=trainprob, testprob=testprob)
  return(output)
}

################ 
## Calculate and plot an ROC with CI and optimal threshold
################ 
calcAUC_plot <- function(obs, probpred, xptext, yptext, icolors, printauc, atitle){
  library(pROC)
  ROC <- plot.roc(obs, 
                  probpred,
                  ci=TRUE, print.auc=printauc, print.auc.x=xptext, print.auc.y=yptext,
                  col=icolors, lty=1, legacy.axes=TRUE,
                  main=atitle)
  # determine best operating point (maximizes both Se Spe)
  # ìbestî: the threshold with the highest sum sensitivity + specificity is plotted (this might be more than one threshold).
  best_thr=ci(ROC, of="thresholds", thresholds="best")
  plot(best_thr, col=icolors) # add one threshold
  #print(ROC$auc)
  #print(best_thr)
  output <- list(ROC=ROC, auc=ROC$auc, best_thr=best_thr)
  return(output)
}

###################################################
### code for creating an ensemble of RF
###statsAU
###################################################
create_ensemble <- function(dat, particvfoldK, cvK){
  #inint
  library(pROC)
  ensemblegrdperf=list()
  maxM = list()
  for(r in 1:cvK){
    ## pick one of cvfold for held-out test, train on the rest
    kparti_setdata = kparti_sample(dat, particvfoldK, cvK, r)
    
    # Boruta on $cvTrainsetD
    selfeatures_kfold = subset_select(kparti_setdata$cvTrainsetD)
    names(selfeatures_kfold)
    
    ###################################################
    # create grid of evaluation points
    gT = c(5,10,30,60,100,250,500,750) 
    gD = c(2,5,10) 
    grd <- expand.grid(x = gD, y = gT)
    
    ###################################################
    # for oneshot
    grdperf = data.frame(grd)
    grdperf$acuTrain =0
    grdperf$rocTrain =0
    grdperf$senTrain =0
    grdperf$speTrain =0
    
    grdperf$acuTest =0
    grdperf$rocTest =0
    grdperf$senTest =0
    grdperf$speTest =0
    
    M = list()
    for(k in 1:nrow(grd)){
      D=grd[k,1]
      T=grd[k,2]
      # Build in l
      cat("D: ", D, "T: ", T, "\n")
      TrainsetD <-  kparti_setdata$cvTrainsetD[c(names(selfeatures_kfold))]
      TestsetD <-  kparti_setdata$cvTestsetD[c(names(selfeatures_kfold))]
      fit <- rpart_looforestTrain(T, D, TrainsetD)
      # # predict
      perf <- rpart_looforestTest(T, TrainsetD, TestsetD, fit$forest)
      # for train
      ROCF_train <- roc(perf$trainprob$obs, perf$trainprob[,1], plot=FALSE,
                        col="#000086", main=paste0("ROC T=",T," D=",D," cv=",r))
      print(ROCF_train$auc)
      # collect data
      grdperf$acuTrain[k] = grdperf$acuTrain[k]+as.numeric(perf$etrain$overall[1])
      grdperf$rocTrain[k] = grdperf$rocTrain[k]+as.numeric(ROCF_train$auc)
      grdperf$senTrain[k] = grdperf$senTrain[k]+as.numeric(perf$etrain$byClass[1])
      grdperf$speTrain[k] = grdperf$speTrain[k]+as.numeric(perf$etrain$byClass[2])
      # for test
      plot.new()
      ROCF_test <- roc(perf$testprob$obs, perf$testprob[,1], plot=FALSE,
                       col="#860000", main=paste0("ROC T=",T," D=",D," cv=",r))
      legend("bottomright", legend = c("train", "test"), col = c("#000086", "#860000"),lwd = 2)
      print(ROCF_test$auc)
      # collect data
      grdperf$acuTest[k] = grdperf$acuTest[k]+as.numeric(perf$etest$overall[1])
      grdperf$rocTest[k] = grdperf$rocTest[k]+as.numeric(ROCF_test$auc)
      grdperf$senTest[k] = grdperf$senTest[k]+as.numeric(perf$etest$byClass[1])
      grdperf$speTest[k] = grdperf$speTest[k]+as.numeric(perf$etest$byClass[2])
      # append perfm for ROC
      M = append(M, list(M = list(D = D, T = T, trainprob = perf$trainprob, 
                                  testprob = perf$testprob, forest = fit$forest)))
    }
    
    pander(grdperf)
    index = which(grdperf$rocTest == max(grdperf$rocTest), arr.ind = TRUE)[1]
    Dmax = grdperf$x[index]
    Tmax = grdperf$y[index]
    resamMax = M[index]$M$testprob
    # append
    maxM <- append(maxM, list(maxp = list(D = Dmax, T = Tmax, 
                                          trainprob = M[index]$M$trainprob, 
                                          testprob = M[index]$M$testprob, 
                                          forest = M[index]$M$forest)))
    ensemblegrdperf <- append(ensemblegrdperf, list(grdperf = grdperf))
  }
  output <- list(ensemblegrdperf = ensemblegrdperf, maxM = maxM)
  return(output)
}

###################################################
### code to plot learning surfaces ncol=2
###################################################
surface_forestperfm <- function(grdperf, ncol, dim1, dim2){
  library(gridExtra) 
  library(base)
  library(lattice)
  
  graphlist<-list()
  count <- 1
  # design rocTrain
  z = grdperf[,1]
  gD=unique(grdperf[,c(dim1)])
  gT=unique(grdperf[,c(dim2)])
  dim(z) <- c(length(gD), length(gT))
  w1 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = dim1,
                  ylab = dim2,
                  main = "Influence of parameters on ROC AUC 5f.cv Train",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  zlim=c(0.5,1),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w1
  count <- count+1
  
  # design rocTest
  z = grdperf[,2]
  dim(z) <- c(length(gD), length(gT))
  w2 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = dim1,
                  ylab = dim2,
                  main = "Influence of parameters on ROC AUC 5f.cv Val",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  zlim=c(0.5,1),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w2
  count <- count+1
  
  # finally plot in grid
  do.call("grid.arrange",c(graphlist,ncol=ncol))
}

###################################################
```


## Create ensembles of RF to compare with unsupervised learning
```{r echo=TRUE, eval=TRUE, message = FALSE, warning=FALSE}
# Read CSV into R
library(pROC)
setwd("Z:/Cristina/Section3/paper_notes/comparison_traditionalCAD")
source('functionsCAD.R')
pdatalabels <- read.csv(file="input/pdatalabels.csv", header=TRUE, sep=",")
dyn_roi <- read.csv(file="input/dyn_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")
morpho_roi <- read.csv(file="input/morpho_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")
text_roi <- read.csv(file="input/text_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")
stage1_roi <- read.csv(file="input/stage1_roi_records_allNMEs_descStats.csv", header=TRUE, sep=",")

nxGnorm <- read.csv(file="input/nxGnormfeatures_allNMEs_descStats.csv", header=FALSE, sep=",")
colnames(nxGnorm) <- paste("nxg", c(1:ncol(nxGnorm)),  sep ="")

# append all with lables
allfeatures = data.frame(cbind(pdatalabels,
                               dyn_roi[2:ncol(dyn_roi)],
                               morpho_roi[2:ncol(morpho_roi)],
                               text_roi[2:ncol(text_roi)],
                               stage1_roi[2:ncol(stage1_roi)]))

# print summary labesl
summary(allfeatures$labels)

# remove unlabeled or 'K'
# print summary of lesions in dataset:
onlylab_nxG =  cbind(allfeatures[allfeatures$labels!='K',], nxGnorm[allfeatures$labels!='K',])
onlylab_nxG$labels = factor(onlylab_nxG$labels)
summary(onlylab_nxG$labels)

## normalize data before training a neural network on it
onlylab_nxG$dce2SE19[is.na(onlylab_nxG$dce2SE19)] <- summary(onlylab_nxG$dce2SE19)[[4]]
onlylab_nxG$dce3SE19[is.na(onlylab_nxG$dce3SE19)] <- summary(onlylab_nxG$dce3SE19)[[4]]
onlylab_nxG$earlySE19[is.na(onlylab_nxG$earlySE19)] <- summary(onlylab_nxG$earlySE19)[[4]]

## split in 90%/10% train/test 
sep = round(nrow(allfeatures)*0.10)
X_test = onlylab_nxG[1:sep,]
y_test = X_test$labels
init_indTrain = sep+1
X_train = onlylab_nxG[init_indTrain:nrow(onlylab_nxG),]
y_train = X_train$labels


###################################################
### Train a CAD classifier using traditional CAD features (only supervised features)
###################################################
## create stratified kfolds
cvK = 5
particvfoldK = cvfold_partition(X_train, cvK)

###################################################
# create grid of evaluation points
gT = c(25,100,250,500,750,1500) 
gD = c(1,3,5) 
grd <- expand.grid(D = gD, ntrees = gT)

###################################################
# initialize grid search metrics
grdperf = data.frame(grd)
grdperf$avaucTrain =0
grdperf$stdTrain =0
grdperf$avaucVal =0
grdperf$stdVal =0

for(k in 1:nrow(grd)){
  # get grid search cell
  D=grd[k,1]
  ntrees=grd[k,2]
  # Build in l
  cat("Depth: ", D, "ntrees: ", ntrees, "\n")
  cvAUC_train = c()
  cvAUC_val = c()
  for(r in 1:cvK){
    ## pick one of cvfold for held-out test, train on the rest
    kparti_setdata = kparti_sample(X_train, particvfoldK, cvK, r)
    
    # Boruta on $cvTrainsetD
    selfeatures_kfold = subset_select(kparti_setdata$cvTrainsetD)
    names(selfeatures_kfold)
    
    # train classifier in train with featset in train
    TrainsetD <-  kparti_setdata$cvTrainsetD[c(names(selfeatures_kfold))]
    ValsetD <-  kparti_setdata$cvValsetD[c(names(selfeatures_kfold))]
    
    # for grid search parameters train
    fit <- rpart_looforestTrain(ntrees, D, TrainsetD)
    # # predict and evaluate performance
    perf <- rpart_looforestTest(ntrees, TrainsetD, ValsetD, fit$forest)
    # for train
    ROC_train <- roc(perf$trainprob$obs, perf$trainprob[,1], plot=FALSE)
    ROC_val <- roc(perf$testprob$obs, perf$testprob[,1], plot=FALSE)
    print(paste0("ROC_train$auc = ",ROC_train$auc))
    print(paste0("ROC_val$auc = ",ROC_val$auc))
    # appends
    cvAUC_train = c(cvAUC_train, ROC_train$auc)
    cvAUC_val = c(cvAUC_val, ROC_val$auc)
  }
  
  # collect data
  grdperf$avaucTrain[k] = mean(cvAUC_train)
  grdperf$stdTrain[k] = sd(cvAUC_train)
  grdperf$avaucVal[k] = mean(cvAUC_val)
  grdperf$stdVal[k] = sd(cvAUC_val)
}

print(grdperf)
################
# plot learning surface of ensemble parameters
grdperf_toplot = grdperf[c(3,5,1,2)]
surface_forestperfm(grdperf_toplot, 2, "D", "ntrees")

################
# select best grid parameters
index = which(grdperf$avaucVal == max(grdperf$avaucVal), arr.ind = TRUE)[1]
Dmax = grdperf$D[index]
nTmax = grdperf$ntrees[index]

maxG = list()
# to pool data
train_pr=data.frame()
val_pr=data.frame()
for(r in 1:cvK){
  ## pick one of cvfold for held-out test, train on the rest
  kparti_setdata = kparti_sample(X_train, particvfoldK, cvK, r)
  
  # Boruta on $cvTrainsetD
  selfeatures = subset_select(kparti_setdata$cvTrainsetD)
  names(selfeatures)
  
  # train classifier in train with featset in train
  TrainsetD <-  kparti_setdata$cvTrainsetD[c(names(selfeatures))]
  ValsetD <-  kparti_setdata$cvValsetD[c(names(selfeatures))]
  
  # for grid search parameters train
  fit <- rpart_looforestTrain(nTmax, Dmax, TrainsetD)
  # # predict and evaluate performance
  perf <- rpart_looforestTest(nTmax, TrainsetD, ValsetD, fit$forest)
  # for train
  ROC_train <- roc(perf$trainprob$obs, perf$trainprob$M, plot=FALSE)
  ROC_val <- roc(perf$testprob$obs, perf$testprob[,1], plot=FALSE)
  print(paste0("ROC_train$auc = ",ROC_train$auc))
  print(paste0("ROC_val$auc = ",ROC_val$auc))
  
  # append
  train_pr = rbind(train_pr, perf$trainprob)
  val_pr = rbind(val_pr, perf$testprob)
  
  # appends forest trees
  maxG = append(maxG, fit$forest)
}

# # predict and evaluate performance
# assess on held out test
perfinal <- rpart_looforestTest(nTmax, X_train, X_test, maxG)

## plot ROC 
plot.new()
ROCcv_train <- plot.roc(train_pr$obs, train_pr$M, col="#000086", lty=1)
ciobj <- ci.se(ROCcv_train, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
ROCcv_train <- plot.roc(train_pr$obs, train_pr$M, col="#000086", lty=1, main="ROC for cvTrain")
legend("bottomright", 
       legend = c(paste0("cvTrain: AUC=", formatC(ROCcv_train$auc,digits=2, format="f"))), 
       col = c("#000086"),lwd = 2, lty = c(1))

plot.new()
ROCcv_val <- plot.roc(val_pr$obs, val_pr$M, col="#008600", lty=1)
ciobj <- ci.se(ROCcv_val, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
ROCcv_val <- plot.roc(val_pr$obs, val_pr$M, col="#008600", lty=1, main="ROC for cvVal")
legend("bottomright", 
       legend = c(paste0("cvVal: AUC=", formatC(ROCcv_val$auc,digits=2, format="f"))), 
       col = c("#008600"),lwd = 2, lty = c(1))

plot.new()
#ROC_test <- plot.roc(perfinal$testprob$obs, perfinal$testprob$B, col="#860000", lty=1)
#ciobj <- ci.se(ROC_test, specificities=seq(0, 1, 0.05)) 
#par(new=TRUE)
#plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
ROC_test <- plot.roc(perfinal$testprob$obs, perfinal$testprob$B, col="#860000", lty=1, main="ROC for held-out test")
legend("bottomright", 
       legend = c(paste0("Test: AUC=", formatC(ROC_test$auc,digits=2, format="f"))), 
       col = c("#860000"),lwd = 2, lty = c(1))

print(ROCcv_train$auc)
print(ROCcv_val$auc)
print(ROC_test$auc)

## significance testing between AUCs
roc.test(ROCcv_train,ROCcv_val,method="bootstrap",boot.stratified = TRUE, alternative="greater")
roc.test(ROCcv_val,ROC_test,method="bootstrap",boot.stratified = TRUE, alternative="greater")

###############################
### now with DEC +MLP classifier
pdZ_grdperf <- read.csv(file="input/pdAUC_Zlatent.csv", header=TRUE, sep=",")
print(pdZ_grdperf)

################
# plot learning surface of ensemble parameters
# assumes the first two columns are AUCtrain, AUCtest
surface_forestperfm(pdZ_grdperf, 2, "spaceD_Redx", "num_clusters")

pdZ_pooled_pred_train <- read.csv(file="input/pooled_pred_train.csv", header=TRUE, sep=",")
pdZ_pooled_pred_val <- read.csv(file="input/pooled_pred_val.csv", header=TRUE, sep=",")
pdZ_test <- read.csv(file="input/pdZ_test.csv", header=TRUE, sep=",")
pdZ_pooled_pred_train$labels = factor(ifelse(pdZ_pooled_pred_train$labels==1,"M","B"))
pdZ_pooled_pred_val$labels = factor(ifelse(pdZ_pooled_pred_val$labels==1,"M","B"))
pdZ_test$labels = factor(ifelse(pdZ_test$labels==1,"M","B"))

## plot ROC 
plot.new()
Z_ROCcv_train <- plot.roc(pdZ_pooled_pred_train$labels, pdZ_pooled_pred_train$probC, col="#000086", lty=1)
ciobj <- ci.se(Z_ROCcv_train, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
Z_ROCcv_train <- plot.roc(pdZ_pooled_pred_train$labels, pdZ_pooled_pred_train$probC, col="#000086", lty=1, main="ROC for cvTrain")
legend("bottomright", 
       legend = c(paste0("cvTrain: AUC=", formatC(Z_ROCcv_train$auc,digits=2, format="f"))), 
       col = c("#000086"),lwd = 2, lty = c(1))

plot.new()
Z_ROCcv_val <- plot.roc(pdZ_pooled_pred_val$labels, pdZ_pooled_pred_val$probC, col="#008600", lty=1)
ciobj <- ci.se(Z_ROCcv_val, specificities=seq(0, 1, 0.05)) 
par(new=TRUE)
plot(ciobj, type="shape", col="grey") # plot as a blue shape
par(new=TRUE)
Z_ROCcv_val <- plot.roc(pdZ_pooled_pred_val$labels, pdZ_pooled_pred_val$probC, col="#008600", lty=1, main="ROC for cvVal")
legend("bottomright", 
       legend = c(paste0("cvVal: AUC=", formatC(Z_ROCcv_val$auc,digits=2, format="f"))), 
       col = c("#008600"),lwd = 2, lty = c(1))

plot.new()
Z_ROCtest <- plot.roc(pdZ_test$labels, pdZ_test$probC, col="#860000", lty=1, main="ROC for held-out")
legend("bottomright", 
       legend = c(paste0("Test: AUC=", formatC(Z_ROCtest$auc,digits=2, format="f"))), 
       col = c("#860000"),lwd = 2, lty = c(1))

print(Z_ROCcv_train$auc)
print(Z_ROCcv_val$auc)
print(Z_ROCtest$auc)

## significance testing between AUCs
roc.test(Z_ROCcv_train, Z_ROCcv_val, method="bootstrap",boot.stratified = TRUE, alternative="greater")
roc.test(Z_ROCcv_train, Z_ROCtest, method="bootstrap",boot.stratified = TRUE, alternative="greater")

# compare two methods on pooled predictions from all data
roc.test(ROCcv_val, Z_ROCcv_val, method="bootstrap",boot.stratified = TRUE, alternative="less")


save.image("Z:/Cristina/Section3/paper_notes/comparison_traditionalCAD/traditionalCADperf_wgraph_gsplits.RData")

```

